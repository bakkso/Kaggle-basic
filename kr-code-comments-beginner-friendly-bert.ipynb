{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"ê°œì¸ ê³µë¶€ ëª©ì ìœ¼ë¡œ BERTëª¨ë¸ì„ ì´ìš©í•œ Baseline ì½”ë“œì— ì£¼ì„ì„ í•œ ì¤„ì”© ë‹¬ì•„ë´¤ìŠµë‹ˆë‹¤.  \n\n\nreference : https://www.kaggle.com/code/suraj520/beginner-friendly-bert  \n\n\nFor my own learning, I've provided Korean comments on each line of the baseline code","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:#5D73F2; color:#19180F; font-size:40px; font-family:Arial; padding:10px; border: 5px solid #19180F; border-radius:10px\"> Beginner friendly approach </div>\n<div style=\"background-color:#D5D9F2; color:#19180F; font-size:15px; font-family:Arial; padding:10px; border: 5px solid #19180F; border-radius:10px\">\n<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nğŸ“Œ\nBERT based approach. Know more about the architecture of BERT via <a href=\"https://www.kaggle.com/code/suraj520/bert-know-fit-infer\"> kernel </a>   </div>\n</div>","metadata":{}},{"cell_type":"markdown","source":"\n<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nğŸ“Œ\nImporting modules\n    </div>","metadata":{}},{"cell_type":"code","source":"# ëª¨ë“ˆ ì„í¬íŠ¸\nimport torch\nimport torch.nn as nn\nfrom transformers import BertModel, BertTokenizer\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nğŸ“Œ\nLoading the data    </div>","metadata":{}},{"cell_type":"code","source":"# ë°ì´í„° ë¡œë“œ\ntrain_data = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_train.csv')\ntest_data = pd.read_csv('/kaggle/input/commonlit-evaluate-student-summaries/summaries_test.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nğŸ“Œ\nPreprocessing the data    </div>","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('/kaggle/input/hugging-face-models-safe-tensors/bert-base-uncased')\n\"\"\"\nBertTokenizerëŠ” Hugging Faceì˜ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” BERT ëª¨ë¸ì„ ìœ„í•œ í† í¬ë‚˜ì´ì €ë¡œ, í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í˜•íƒœì˜ í† í°ìœ¼ë¡œ ë³€í™˜í•œë‹¤.\nfrom_pretrained() : íŠ¹ì • ëª¨ë¸ì„ ì¸ìë¡œ ì œê³µí•˜ë©´ í•´ë‹¹ ëª¨ë¸ì˜ í† í¬ë‚˜ì´ì € config ë° vocabì„ ë¡œë“œí•  ìˆ˜ ìˆë‹¤.\n\"\"\"\n\n\n\ntrain_encodings = tokenizer.batch_encode_plus(\n    train_data['text'].tolist(),\n    truncation=True,\n    padding=True\n)                                 \n\"\"\"\n- batch_encode_plusëŠ” ì—¬ëŸ¬ê°œì˜ í…ìŠ¤íŠ¸ ë¬¸ì¥ì„ í•œ ë²ˆì— ì¸ì½”ë”©í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤.\n- tolist()ëŠ” pandasì˜ DataFrameì´ ì œê³µí•˜ëŠ” ë©”ì„œë“œë¡œ, í•´ë‹¹ ë°ì´í„°ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•œë‹¤.\n- tolist()ë¥¼ í•˜ëŠ” ì´ìœ : batch_encode_plusëŠ” ì…ë ¥ìœ¼ë¡œ Python ë¦¬ìŠ¤íŠ¸, íŠœí”Œ ë˜ëŠ” ë‹¤ë¥¸ ì‹œí€€ìŠ¤ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë°›ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ\n- turncation = True : í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ê°€ ëª¨ë¸ì˜ max lengthë¥¼ ì´ˆê³¼í•  ê²½ìš°, í…ìŠ¤íŠ¸ë¥¼ ì˜ë¼ë‚¸ë‹¤.\n- padding = True : ì¸ì½”ë”©ëœ í…ìŠ¤íŠ¸ê°€ ë™ì¼í•œ ê¸¸ì´ë¥¼ ê°–ë„ë¡ íŒ¨ë”©ì„ ì¶”ê°€í•œë‹¤.\n\"\"\"\n\n# test dataì—ë„ ë™ì¼í•˜ê²Œ ì ìš©\ntest_encodings = tokenizer.batch_encode_plus(\n    test_data['text'].tolist(),\n    truncation=True,\n    padding=True\n)\n\n\ntrain_dataset = torch.utils.data.TensorDataset(\n    torch.tensor(train_encodings['input_ids']),\n    torch.tensor(train_encodings['attention_mask']),\n    torch.tensor(train_data['content'].tolist()),\n    torch.tensor(train_data['wording'].tolist())\n)\n\"\"\"\n- torch.utils.data.TensorDataset : í…ì„œë“¤ì˜ ë°ì´í„°ì…‹ì„ ë§Œë“ ë‹¤.\n- tokenizer.batch_encode_plus()ë¥¼ ì‚¬ìš©í•´ì„œ ì¸ì½”ë”©í•œ ê²°ê³¼ì—ì„œ 'input_ids'ë¥¼ ê°€ì ¸ì˜¨ë‹¤. \n- 'input_ids'ëŠ” í† í°í™”ëœ í…ìŠ¤ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ«ì IDì˜ ë¦¬ìŠ¤íŠ¸ì´ë‹¤.\n- 'attention_mask'ëŠ” 0,1ë¡œ êµ¬ì„±ë¼ìˆëŠ”ë°, ì‹¤ì œ í† í°ì€ 1, íŒ¨ë”© í† í°ì€ 0ì˜ ê°’ì„ ê°€ì§„ë‹¤.\n- .tolist() : torch.tensorê°€ pandasì˜ Seriesê°ì²´ë¥¼ ì§ì ‘ ì²˜ë¦¬í•  ìˆ˜ ì—†ì–´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n- wordingë„ ë§ˆì°¬ê°€ì§€ \n\"\"\"\n\n# test dataì—ë„ ë™ì¼í•˜ê²Œ ì ìš©\ntest_dataset = torch.utils.data.TensorDataset(\n    torch.tensor(test_encodings['input_ids']),\n    torch.tensor(test_encodings['attention_mask'])\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_dataset ë‚´ìš© í™•ì¸\n\nprint(\"ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ Inputs ids\",train_dataset[0][0])\nprint(\"ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ attention Mask\", train_dataset[0][1])\nprint(\"ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ 'content'\",train_dataset[0][2])\nprint(\"ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ 'wording'\",train_dataset[0][3])\n\nprint(\"ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ Inputs ids shape \",train_dataset[0][0].shape)\nprint(\"ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ attention Mask shape\", train_dataset[0][1].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nğŸ“Œ\nDefining the BERT model    </div>","metadata":{}},{"cell_type":"code","source":"class BERTModel(nn.Module):  # Pytorchì˜ nn.Module í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ì„œ ì‚¬ìš©í•œë‹¤.\n    def __init__(self):\n        super(BERTModel, self).__init__()\n        self.bert = BertModel.from_pretrained('/kaggle/input/hugging-face-models-safe-tensors/bert-base-uncased') # pretrainëœ BERT ëª¨ë¸ ë¡œë“œ\n\n        self.dropout = nn.Dropout(0.1) # overfitting ë°©ì§€ë¥¼ ìœ„í•œ ë“œë¡­ì•„ì›ƒ\n        self.linear1 = nn.Linear(768, 256) # ì²« ë²ˆì§¸ ì„ í˜• ë ˆì´ì–´. ì…ë ¥ í¬ê¸°ëŠ” 768(BERT ëª¨ë¸ì˜ íˆë“  ë ˆì´ì–´ í¬ê¸°), ì¶œë ¥ í¬ê¸°ëŠ” 256\n        self.linear2 = nn.Linear(256, 2)  # ë‘ ë²ˆì§¸ ì„ í˜• ë ˆì´ì–´. ì…ë ¥ í¬ê¸°ëŠ” 256, ì¶œë ¥ í¬ê¸°ëŠ” 2\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)  # input_idsì™€ attention_maskë¥¼ BERTëª¨ë¸ì— ì „ë‹¬í•˜ê³ , ê·¸ ì¶œë ¥ì€ outputsì— ë‹´ëŠ”ë‹¤.\n        \n        pooled_output = outputs.pooler_output\n        pooled_output = self.dropout(pooled_output)\n        \"\"\"\n        pooler_outputì´ë€? BERTì˜ ë§ˆì§€ë§‰ Transformer ê³„ì¸µì—ì„œ [CLS] í† í°ì˜ ì¶œë ¥ì„ ê°€ì ¸ì™€ì„œ\n        ì¶”ê°€ì ì¸ Dense ë ˆì´ì–´ë¥¼ í†µê³¼ì‹œí‚¨ ê²°ê³¼ì´ë‹¤.\n        ì´ CLS í† í°ì˜ ì„ë² ë”©ì€ ì „ì²´ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ë¬¸ë§¥ì  ìš”ì•½ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.\n        ë”°ë¼ì„œ ì´ CLS í† í°ì˜ ì„ë² ë”©(pooler_output)ì„ ì´ìš©í•´ì„œ ì¶”ê°€ ì‘ì—…ì„ í•œë‹¤.\n        \"\"\"\n        \n        \n        output = self.linear1(pooled_output) # ì²« ë²ˆì§¸ ì„ í˜• ë ˆì´ì–´ í†µê³¼\n        output = nn.ReLU()(output)  # activation functionì¸ ReLU ì ìš©\n        output = self.linear2(output)  # ë‘ ë²ˆì§¸ ì„ í˜• ë ˆì´ì–´ í†µê³¼\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nğŸ“Œ\nTraining the BERT model    </div>","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # CUDAê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ deviceëŠ” 'cuda'ë¡œ ì„¤ì •ë˜ê³ , ì•„ë‹ˆë©´ 'cpu'ë¡œ ëœë‹¤.\nmodel = BERTModel().to(device) # ì•ì—ì„œ ì •ì˜í•œ BERTModel í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ì¸ modelì„ ìƒì„±í•˜ê³ , .to(device)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ëª¨ë¸ì„ deviceë¡œ ì´ë™ì‹œí‚¨ë‹¤.\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5) # optimizer ì •ì˜. model.parameters()ëŠ” ìë™ìœ¼ë¡œ ëª¨ë¸ ë‚´ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì ¸ì˜¨ë‹¤.\ncriterion = nn.MSELoss()  # loss functionìœ¼ë¡œ Mean Squared Error ì‚¬ìš©.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nğŸ“Œ\nCreating data loader and performing sanity check    </div>","metadata":{}},{"cell_type":"code","source":"batch_size= 16  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting training data into train and validation sets\ntrain_dataset, val_dataset = train_test_split(train_dataset, test_size=0.2, random_state=0)\n# ë°ì´í„°ë¥¼ trainìš© ë°ì´í„°ì™€ validation ë°ì´í„°ë¡œ ë‚˜ëˆˆë‹¤. train_datasetì€ 80%ë¡œ, val_datsetì€ 20%ë¡œ\n\n\n# Creating train loader\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# Creating validation loader\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\"\"\"\ntorch.utils.data.DataLoaderëŠ” Pytorchì—ì„œ ì œê³µí•˜ëŠ” DataLoader\në°ì´í„° ë¡œë”ë¥¼ ì´ìš©í•˜ë©´ ë°ì´í„°ë¥¼ ì‰½ê²Œ ë°°ì¹˜ë¡œ ë¶„í• í•˜ì—¬ ëª¨ë¸ trainì´ë‚˜ validationì— ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.\nì¼ë°˜ì ìœ¼ë¡œ train dataëŠ” shuffleì„ í•˜ê³ , ê²€ì¦, testëŠ” shuffleì„ ì•ˆ í•œë‹¤.\n\"\"\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for batch in train_loader:\n    print(batch)\n    \n\"\"\"\nì—¬ê¸°ì„œ ê°ê°ì˜ batchê°€ ëœ»í•˜ëŠ” ê²ƒ : (input_ids, attention_mask, content, wording)\nê·¸ë˜ì„œ len(batch) ì°ìœ¼ë©´ ëª¨ë‘ 4ê°€ ë‚˜ì˜¨ë‹¤.\n\"\"\"","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nğŸ“Œ\nTraining the model for 30 epochs    </div>","metadata":{}},{"cell_type":"code","source":"# Training loop\nmodel.train()  # modelì„ train ëª¨ë“œë¡œ ì„¤ì •.\nfor epoch in range(30):  # 30 epochë™ì•ˆ ëª¨ë¸ì„ í›ˆë ¨í•œë‹¤\n    running_loss = 0.0  # í˜„ì¬ epochì—ì„œ ë°œìƒí•œ ëª¨ë“  ë°°ì¹˜ì˜ lossê°’ì„ ëˆ„ì í•˜ê¸° ìœ„í•œ ë³€ìˆ˜ running_loss\n    for step, (input_ids, attention_mask, content, wording) in enumerate(train_loader):  # train_loaderë¡œë¶€í„° ë°°ì¹˜ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì„œ ê° ë°°ì¹˜ì— ëŒ€í•´ì„œ ì•„ë˜ì˜ ì½”ë“œë¥¼ ì‹¤í–‰\n        \n        # í•´ë‹¹ ë°°ì¹˜ì˜ ë°ì´í„°ë¥¼ GPUë¡œ ì „ì†¡\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        content = content.to(device)\n        wording = wording.to(device)\n\n        optimizer.zero_grad()  # ì´ì „ ë°°ì¹˜ì—ì„œì˜ ê¸°ìš¸ê¸° ì •ë³´ ì´ˆê¸°í™”\n\n        outputs = model(input_ids, attention_mask)  # í˜„ì¬ ë°°ì¹˜ì˜ dataë¥¼ ëª¨ë¸ì— ì§‘ì–´ë„£ê³ , outputsì„ ë°›ëŠ”ë‹¤. outputsëŠ” [batch_size, 2]ê°€ ë  ê²ƒì´ë‹¤.\n        \n        loss = criterion(outputs[:, 0], content) + criterion(outputs[:, 1], wording) # outputsì„ ì‚¬ìš©í•´ì„œ loss ê³„ì‚°. ì²« ë²ˆì§¸ ê°’ì„ contentì™€, ë‘ ë²ˆì§¸ ê°’ì„ wordingê³¼ ë¹„êµí•œë‹¤.\n        loss.backward()  # lossì— ëŒ€í•œ gradient ê³„ì‚°\n        optimizer.step()  # ê³„ì‚°ëœ gradientë¥¼ ì‚¬ìš©í•´ì„œ parameter ì—…ë°ì´íŠ¸\n        \n        if step % 500 == 0:  # 500 stepë§ˆë‹¤ epoch, step, loss ì¶œë ¥\n            print(\"Epoch {}, Step {}, Loss: {}\".format(epoch+1, step, loss.item()))\n\n        running_loss += loss.item()  # í˜„ì¬ ë°°ì¹˜ì˜ lossë¥¼ running_lossì— ë”í•œë‹¤.\n\n    print(f\"Epoch {epoch+1} Loss: {running_loss / len(train_loader)}\")  # ê° epoch ì´í›„ì— í‰ê·  train lossë¥¼ ì¶œë ¥\n\n    # Validation loop\n    model.eval()  # ëª¨ë¸ì„ validation ëª¨ë“œë¡œ ì„¤ì •\n    with torch.no_grad():  # gardient ê³„ì‚° ì¤‘ì§€ (validationì´ë¯€ë¡œ)\n        val_loss = 0.0  # trainì—ì„œì™€ ë™ì¼í•˜ê²Œ ì§„í–‰, ê·¼ë° parameter ì—…ë°ì´íŠ¸ ê³¼ì •ì´ ì—†ìŒ\n        \n        # validation  dataì˜ ê° ë°°ì¹˜ì— ëŒ€í•´ì„œ ì‹¤í–‰\n        for val_step, (input_ids, attention_mask, content, wording) in enumerate(val_loader):\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            content = content.to(device)\n            wording = wording.to(device)\n\n            val_outputs = model(input_ids, attention_mask)\n            val_loss += criterion(val_outputs[:, 0], content) + criterion(val_outputs[:, 1], wording)\n\n        print(f\"Validation Loss: {val_loss / len(val_loader)}\")\n    model.train()  # ë‹¤ìŒ epochë¥¼ ìœ„í•´ì„œ ë‹¤ì‹œ train ëª¨ë“œë¡œ ì„¤ì •í•œë‹¤.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nğŸ“Œ\nCreating test loader    </div>","metadata":{}},{"cell_type":"code","source":"# test datasetì„ ìœ„í•œ test ë°ì´í„° ë¡œë”ë¥¼ ë§Œë“ ë‹¤.\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nğŸ“Œ\nGenerating predictions on test set    </div>","metadata":{}},{"cell_type":"code","source":"model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜\npredictions = []  # ì˜ˆì¸¡ê°’ì„ ì €ì¥í•  ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìƒì„±\nwith torch.no_grad():  # gradient ê³„ì‚° x\n    for input_ids, attention_mask in test_loader: # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë”ì—ì„œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ input_idsì™€ attention_maskë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.\n        \n        # GPUì— ë°ì´í„° ë‹´ê¸°\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n\n        outputs = model(input_ids, attention_mask) # ëª¨ë¸ì— input_idsì™€ attention_mask ë„£ê³  Predication ìˆ˜í–‰. outputsì—ëŠ” í•´ë‹¹ ë°°ì¹˜ì˜ ëª¨ë¸ prediction ê°’ì´ ì €ì¥ë˜ì–´ ìˆë‹¤.\n        \n        predictions.extend(outputs.cpu().numpy()) \n        \"\"\" \n        outputs í…ì„œë¥¼ cpu()ë¡œ ì˜®ê¸°ê³ , numpy ë°°ì—´ë¡œ ë³€í™˜í•œë‹¤. \n        ê·¸ í›„ predictions ë¦¬ìŠ¤íŠ¸ì— ì´ ê°’ì„ ì¶”ê°€í•œë‹¤. ì™œ CPUë¡œ ì˜®ê¸°ëƒ? \n        .numpy()ë¥¼ ì´ìš©í•´ì„œ Pytorch í…ì„œë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜í• ë•Œ,\n        í•´ë‹¹ í…ì„œëŠ” ê¼­ CPUì— ìˆì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.\"\"\"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<div style=\"background-color:#F0E3D2; color:#19180F; font-size:15px; font-family:Verdana; padding:10px; border: 2px solid #19180F; border-radius:10px\"> \nğŸ“Œ\nGenerating submission\n    </div>","metadata":{}},{"cell_type":"code","source":"submission_df = pd.DataFrame({  # ìƒˆ ë°ì´í„°í”„ë ˆì„ submission_df ìƒì„±\n    'student_id': test_data['student_id'],  # test_dataì—ì„œ 'student_id' ì—´ ê°’ì„ ê°€ì ¸ì™€ì„œ, submission_dfì˜ 'student_id' ì—´ì— ê·¸ëŒ€ë¡œ í• ë‹¹\n    'content': [pred[0] for pred in predictions],  # predictions ë¦¬ìŠ¤íŠ¸ì˜ ê° ì›ì†Œì—ì„œ ì²« ë²ˆì§¸ ê°’(pred[0])ì„ ê°€ì ¸ì™€ì„œ ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ê³ , 'content' ì—´ì— í• ë‹¹\n    'wording': [pred[1] for pred in predictions]  # predictions ë¦¬ìŠ¤íŠ¸ì˜ ê° ì›ì†Œì—ì„œ ë‘ ë²ˆì§¸ ê°’(pred[1])ì„ ê°€ì ¸ì™€ì„œ ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ê³ , 'wording' ì—´ì— í• ë‹¹\n})\n\nsubmission_df.to_csv('submission.csv', index=False)  # dataframe ë‚´ìš©ì  csvíŒŒì¼ë¡œ ì €ì¥, í–‰ ë²ˆí˜¸ëŠ” í¬í•¨í•˜ì§€ ì•Šë„ë¡ index=False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}